import torch
import os
from abc import ABC, abstractmethod

class BaseModel(ABC):
    """
    åŸºç¤æ¨¡å‹é¡ï¼Œæ‰€æœ‰æ¨¡å‹é¡éƒ½æ‡‰è©²ç¹¼æ‰¿è‡ªé€™å€‹é¡
    """
    def __init__(self, model_name):
        self.model_name = model_name
        self.device = self._get_device()
        
    def _get_device(self):
        """ç²å–å¯ç”¨çš„è¨­å‚™ (é‡å° Render éƒ¨ç½²å„ªåŒ–)"""
        # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ï¼Œå¼·åˆ¶ä½¿ç”¨ CPU (é©ç”¨æ–¼ Render ç­‰é›²ç«¯å¹³å°)
        import os
        if os.getenv('FORCE_CPU', 'false').lower() == 'true':
            print("ğŸ”§ ç’°å¢ƒè®Šæ•¸ FORCE_CPU=trueï¼Œå¼·åˆ¶ä½¿ç”¨ CPU")
            return "cpu"
        
        if torch.cuda.is_available():
            print("ğŸš€ æª¢æ¸¬åˆ° CUDAï¼Œä½¿ç”¨ GPU åŠ é€Ÿ")
            return "cuda"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            print("ğŸ æª¢æ¸¬åˆ° MPS (Mac M1)ï¼Œä½¿ç”¨ MPS åŠ é€Ÿ")
            return "mps"
        else:
            print("ğŸ’» ä½¿ç”¨ CPU é‹ç®—")
            return "cpu"
    
    @abstractmethod
    def load_pipeline(self):
        """åŠ è¼‰æ¨¡å‹ç®¡é“ï¼Œå­é¡å¿…é ˆå¯¦ç¾é€™å€‹æ–¹æ³•"""
        pass
    
    def optimize_pipeline(self, pipe):
        """å„ªåŒ–æ¨¡å‹ç®¡é“ï¼Œå•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–ç­‰"""
        # ç§»é™¤å®‰å…¨æª¢æŸ¥å™¨
        pipe.safety_checker = None
        
        # å°‡æ¨¡å‹ç§»è‡³æŒ‡å®šè¨­å‚™
        pipe = pipe.to(self.device)
        
        # ğŸš€ å¼·åŒ–è¨˜æ†¶é«”å„ªåŒ–
        if hasattr(pipe, "enable_attention_slicing"):
            pipe.enable_attention_slicing()
        
        # å˜—è©¦å•Ÿç”¨ CPU å¸è¼‰ (éœ€è¦ accelerate å¥—ä»¶)
        try:
            if hasattr(pipe, "enable_model_cpu_offload") and self.device in ["mps", "cuda"]:
                pipe.enable_model_cpu_offload()
                print(f"âœ… å·²å•Ÿç”¨ {self.device.upper()} CPU å¸è¼‰")
            elif hasattr(pipe, "enable_sequential_cpu_offload") and self.device in ["cuda", "mps"]:
                pipe.enable_sequential_cpu_offload()
                print(f"âœ… å·²å•Ÿç”¨ {self.device.upper()} é †åº CPU å¸è¼‰")
        except Exception as e:
            print(f"âš ï¸ CPU å¸è¼‰ä¸å¯ç”¨ (éœ€è¦å®‰è£ accelerate): {e}")
            print("ğŸ’¡ å»ºè­°åŸ·è¡Œ: pip install accelerate")
        
        # è¨˜æ†¶é«”é«˜æ•ˆæ³¨æ„åŠ›ï¼šMac M1 ä¸æ”¯æ´ xformersï¼Œè·³é
        if self.device != "mps" and hasattr(pipe, "enable_xformers_memory_efficient_attention"):
            try:
                pipe.enable_xformers_memory_efficient_attention()
                print("âœ… å·²å•Ÿç”¨ xformers è¨˜æ†¶é«”é«˜æ•ˆæ³¨æ„åŠ›")
            except Exception as e:
                print(f"âš ï¸ xformers ä¸å¯ç”¨: {e}")
        elif self.device == "mps":
            print("â„¹ï¸ Mac M1 ä¸æ”¯æ´ xformersï¼Œä½¿ç”¨åŸç”Ÿ MPS å„ªåŒ–")
        
        # å•Ÿç”¨ VAE åˆ‡ç‰‡ (æ¸›å°‘ VAE è¨˜æ†¶é«”ä½¿ç”¨)
        if hasattr(pipe, "enable_vae_slicing"):
            pipe.enable_vae_slicing()
        
        # å•Ÿç”¨ VAE å¹³é‹ª (è™•ç†å¤§åœ–åƒæ™‚ç¯€çœè¨˜æ†¶é«”)
        if hasattr(pipe, "enable_vae_tiling"):
            pipe.enable_vae_tiling()
            
        return pipe
    
    def load_lora_weights(self, pipe, weight_name):
        """åŠ è¼‰ LoRA æ¬Šé‡"""
        if weight_name:
            try:
                pipe.load_lora_weights("assets/weights", weight_name=weight_name)
                print(f"âœ… å·²åŠ è¼‰ LoRA æ¬Šé‡ï¼š{weight_name}")
            except Exception as e:
                print(f"âš ï¸ åŠ è¼‰ LoRA æ¬Šé‡å¤±æ•—: {e}")
        return pipe